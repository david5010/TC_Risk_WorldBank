#!/usr/bin/env python
# coding: utf-8

# In[86]:


#reset


# In[87]:


import xarray as xr
import numpy as np
import matplotlib.pyplot as plt
import xesmf as xe
import julian
import datetime
import cartopy.crs as ccrs
import cartopy.feature as cfeature
import pickle
import datetime
from datetime import datetime
from tcrisk.vulnerability import vulnerability_xr
import time

# Python3 code to remove spaces from string
def remove(string):
    return string.replace(" ", "")

# For timing the script
start_time = time.time()

# Root directory: replace with where data downloaded to
root_dir = '/data2/jbaldwin/WCAS2023'

# # Load Philippines region masks & names
# ## Impt note: sets which regions scripts calculates over.

# In[89]:


# rename lat and lon dimensions to be consistent with asset loss data
d_mask = xr.open_dataset(root_dir+'/REGION_MASKS/philippines_province_masks_newlitpopgrid.nc').rename({'longitude':'lon','latitude':'lat'})
pickle_in = open(root_dir+'/REGION_MASKS/state_ids.p',"rb")
state_ids = pickle.load(pickle_in)
regions = list(state_ids.keys())


# # Set directory to save to

# In[88]:


# For both max wind and asset loss data
direc_save = root_dir+'/ASSET_LOSSES/CHAZ/'


# # Set bounding box for Philippines

# In[90]:


#Bounding Box for Philippines
lonmin = 117.17427453
latmin = 5.58100332277
lonmax = 126.537423944
latmax = 18.5052273625


# # Load swath data

# In[91]:


def preprocess(ds):
    dsnew = ds.drop_dims('iT')
    return dsnew

# Swaths for all sets of tracks (0-6)
direc = root_dir+'/HAZARD/WIND_SWATHS/CHAZ/'

# Open series of files
ds = {}
for n in np.arange(7):
    strn = str(n)
    ds[n] = xr.open_mfdataset(direc+'wspd_phi_swaths_00'+strn+'_*.nc',combine='by_coords',preprocess=preprocess,parallel=True)

# Adjust nS coordinate so can concatenate
ds_nScont = {}
ds_nScont[0] = ds[0]
for n in np.arange(1,7,1): # don't need to do for first set (000)-- only from 001 on
    ds_nScont[n] = ds[n].assign_coords(nS = ds[n].nS + ds_nScont[n-1].nS.max() + 1)

# Concatenate all the sets of storms
ds_merged = xr.concat([ds_nScont[0],ds_nScont[1],ds_nScont[2],ds_nScont[3],ds_nScont[4],ds_nScont[5],ds_nScont[6]],dim='nS')

# Select swath variable
swath = ds_merged.swath


# # Load exposed value data

# In[92]:


# Exposed Value Data, subset over Philippines
ds_exp = xr.open_dataset(root_dir+'/EXPOSED_VALUE/litpop_v1-2_phl.nc').sel(lon=slice(lonmin,lonmax),lat=slice(latmin,latmax))
exposed_value = ds_exp.value


# # Regrid swaths to exposed value data

# In[93]:


# Determine Regridder for Hazard --> Exposed Value
regridder = xe.Regridder(swath, ds_exp, 'bilinear',reuse_weights=True)


# Regrid wind swath to exposed value grid
swath_out = regridder(swath)


# # Calculate fractional value lost for the swaths

# In[94]:


# Load regional vulnerability map
Vhalf_map = xr.open_dataset(root_dir+'/VULNERABILITY/Vhalf_map_rmsf_2.nc')

# Set vulnerability curve parameters
Vthresh = 25# Eberenz et al 2020, value for Philippines; m/s
Vhalf = Vhalf_map.states.values # fit for each region
V = swath_out

# Calculate fractional value lose
f = vulnerability_xr(V,Vthresh,Vhalf)


# # Calculate map of asset losses for each historical storm
# We don't save this out to save space.

# In[95]:


# Calculate asset losses
asset_losses = exposed_value*f


# # Load IBTRACS TS per year + threshold 
# For calculating durations of each CHAZ ensemble.

# In[96]:


# Load IBTRACS data of ts_per_year
pickle_in = open(root_dir+"/HAZARD/WIND_SWATHS/IBTrACS/ts_per_year_maxasymcorrec.p","rb")
ts_per_year = pickle.load(pickle_in)

# Wind speed threshold to be a tropical storm
ts_thres = 40/1.944 # 40 kts converted to m/s


# # Loop over regions, calculating and saving out max wind, summed asset losses, and duration for each region

# In[98]:


#%%time
for key in regions:
    print(key)
    
    # Calculate regional mask
    mask0 = d_mask.states.where(d_mask.states==state_ids[key])
    mask = mask0/mask0
    
    # Calculate max wind in each region for each storm
    max_wind = (swath_out*mask).max(dim='lon',skipna = True).max(dim='lat', skipna = True)
    
    # Calculate number of tropical storms in each region
    ts_count = (max_wind>ts_thres).sum(dim='nS')
    
    # Determine period that is consistent with historical IBTRACS ts per year
    # Exception for ARMM because there are no historical storms there (so take rate from closest adjoining region with lowest landfall rate == Soccksksargen)
    if key == 'ARMM':
        dyear = ts_count/ts_per_year['Soccsksargen']
    else:
        dyear = ts_count/ts_per_year['key']
    
    # Mask asset losses over region and sum across region
    asset_losses_reg = (asset_losses*mask).sum(dim='lon',skipna = True).sum(dim='lat', skipna = True)
    
    # Convert max wind, regional asset losses, and duration to datasets
    ds_max_wind = xr.DataArray.to_dataset(max_wind,name='max_wind')
    ds_dyear = xr.DataArray.to_dataset(dyear,name='dyear')
    ds_asset_losses = xr.DataArray.to_dataset(asset_losses_reg,name='asset_losses')
    
    # Merge max wind, regional asset losses, and duration into one dataset
    ds_combined = xr.merge([ds_max_wind,ds_asset_losses,ds_dyear])
    
    # Save out
    ds_combined.to_netcdf(direc_save+remove(key)+'.nc')
    
    
    
# Print out time it took to run
print("My program took", time.time() - start_time, "to run.")

