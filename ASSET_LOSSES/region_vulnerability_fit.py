#!/usr/bin/env python
# coding: utf-8

# In[2]:


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import xarray as xr
import julian
from collections import defaultdict
#import datetime
import matplotlib.pyplot as plt
from datetime import datetime
from scipy.stats import linregress, kendalltau, spearmanr, pearsonr
import pickle
import time

# For timing the script
start_time = time.time()


# Root directory: change to where data downloaded to
root_dir = '/data2/jbaldwin/WCAS2023'

# # EM-DAT data


# Load and format EM-DAT data
data = pd.read_excel(root_dir+'/ASSET_LOSSES/emdat_2021_10_01_philippines_storms.xlsx', skiprows=6)
#data = pd.read_excel(r'/home/jbaldwin/WorldBank/ASSET_LOSSES/emdat_2021_02_10_philippines_TCs.xlsx', skiprows=6)

df = pd.DataFrame(data, columns= ['Event Name','Disaster Subtype','Start Year','Start Month', 'Start Day', 'End Year','End Month', 'End Day', "Total Damages ('000 US$)"])
df = df[~np.isnan(df["Total Damages ('000 US$)"])] # select only data with damages
df = df[df['Disaster Subtype'] != 'Convective storm'] # remove convective storms which are not TCs
df = df[~pd.isna(df['Start Day'])] # remove storms without start days needed for matching with swaths

# Correction: Take Typhoon Faye and Norming and add damages together and create new event
fayenorming = df[df['Event Name'] == 'Faye']
fayenorming.iloc[:,0] = 'Faye (Norming)'
fayenorming.iloc[:,4] = 20
# Combine damages from the two events
fayenorming.iloc[:,8] = df[df['Event Name'] == 'Faye']["Total Damages ('000 US$)"][104] + df[df['Event Name'] == '(Norming)']["Total Damages ('000 US$)"][109]
df.iloc[58] = fayenorming.iloc[0] # replace Faye entry
df = df.drop(index = 109) # remove Norming entry


# EXTRACT DATA

total_damages = np.array(df["Total Damages ('000 US$)"])*1000 # factor in the 1000 here so can ignore

# Dates only for data with damages
# Start and end dates
start_year = df['Start Year'].values*10000
start_month = df['Start Month'].values*100
start_day = df['Start Day'].values
start_date = (start_year+start_month+start_day).astype('int')
start_date = pd.to_datetime(start_date.astype(str),errors='coerce').values
# Start Date with only Year-Month (but all day values seem to be present)
start_date_yearmonth = (start_year+start_month+1).astype('int')
start_date_yearmonth = pd.to_datetime(start_date_yearmonth.astype(str),errors='coerce').values

end_year = df['End Year'].values*10000
end_month = df['End Month'].values*100
end_day = df['End Day'].values
end_date = (end_year+end_month+end_day).astype('int')
end_date = pd.to_datetime(end_date.astype(str),errors='coerce').values
# End Date with only Year-Month because some day values missing
end_date_yearmonth = (end_year+end_month+1).astype('int')
end_date_yearmonth = pd.to_datetime(end_date_yearmonth.astype(str),errors='coerce').values



# # Modeled damage data


# Data files
direc = root_dir
file_swaths = '/HAZARD/WIND_SWATHS/IBTrACS/wspd_phi_swaths_maxasymcorrec_ibtracsv04r00_3-8-21.nc'
file_damages = '/ASSET_LOSSES/IBTrACS/VCURVE_SENSITIVITY/phi_assetlosses_Vhalf-188.4_Vthresh-25.7.nc'


# In[7]:


# Get dates from damage data
ds_damages = xr.open_dataset(direc + file_damages)
nmax = len(ds_damages.nS)
tc_start_date = ds_damages.start_date.values
tc_end_date = ds_damages.end_date.values
year = np.zeros(len(tc_start_date))
for i in range(len(tc_start_date)):
    year[i] = int(str(tc_start_date[i])[0:4])


# In[8]:


#%%time
# Match all with exact date match (Y-m-d)
nemax = len(start_date)
match = np.zeros((nemax,nmax))
for ne in range(nemax):
    if start_date[ne] == np.datetime64('NaT'):
        pass # skip if don't have day value
    else:
        emdat_range = pd.period_range(start_date[ne],end_date[ne])
        for nt in range(nmax):
            tc_range = pd.period_range(tc_start_date[nt],tc_end_date[nt])
            match[ne,nt] = sum(np.isin(tc_range,emdat_range)) # count number of days of overlap


# Select pairs where have greatest number of matches

# Value of greatest overlap
ne_max = np.max(match,axis=1)
ne_max_i = []
nt_max = np.max(match,axis=0)
nt_max_i = []


# Indices of place with greatest overlap
for ne in range(nemax):
    if ne_max[ne] == 0:
        pass
    else:
        ne_max_i.append([ne,np.where(match[ne,:] == ne_max[ne])[0]])
for nt in range(nmax):
    if nt_max[nt] == 0:
        pass
    else:
        nt_max_i.append([nt,np.where(match[:,nt] == nt_max[nt])[0]])

# Divide between clear match and ambiguous (multiple possible matches)
nt_max_i_clear = []
nt_max_i_ambi = []
for nn in range(len(nt_max_i)):
    if len(nt_max_i[nn][1]) == 1:
        nt_max_i_clear.append(np.array([nt_max_i[nn][1][0],nt_max_i[nn][0]]))
    else:
        nt_max_i_ambi.append(nt_max_i[nn]) 
        
ne_max_i_clear = []
ne_max_i_ambi = []
for nn in range(len(ne_max_i)):
    if len(ne_max_i[nn][1]) == 1:
        ne_max_i_clear.append(np.array([ne_max_i[nn][0],ne_max_i[nn][1][0]]))
    else:
        ne_max_i_ambi.append(ne_max_i[nn]) 

# Indices where 1 clear match
ne_pairs = np.stack(ne_max_i_clear,axis=0)
nt_pairs = np.stack(nt_max_i_clear,axis=0)
def intersect2D(a, b):
  """
  Find row intersection between 2D numpy arrays, a and b.
  Returns another numpy array with shared rows
  """
  return np.array([x for x in set(tuple(x) for x in a) & set(tuple(x) for x in b)])
matches = intersect2D(ne_pairs,nt_pairs) # emdat index, tc index

# CORRECTION: append 4 storms that actually have matches but were ambiguous due to close overlap in dates
# from nt_max_i_ambi, and ne_max_i_ambi
# (1995 Angela and Zack, and 2016 Karen and Lawin)
# reconciled by looking at dates of landfall, locations of storms, etc

matches = np.append(matches,[[115,317],[113,318],[230,451],[231,452]],axis=0)


# In[9]:


# LitPop is for 2014
pwt_data = pd.read_excel(root_dir+'/ASSET_LOSSES/pwt100.xlsx', sheet_name = 2)
pwt_df = pd.DataFrame(pwt_data, columns= ['country','countrycode','currency_unit','year','rnna','rgdpna'])
pwt_df = pwt_df[pwt_df['country']=='Philippines'] # subset just data for Philippines
pwt_df['rnna'] = pwt_df['rnna']*1e6
pwt_df['rgdpna'] = pwt_df['rgdpna']*1e6

# Normalize EM-DAT damages to 2014 assets
total_damages_norm = np.full(np.shape(total_damages),np.nan)
baserate = pwt_df['rnna'][pwt_df['year']==2014].values[0]
for i in range(len(total_damages)):
    try:
        yearrate = pwt_df['rnna'][pwt_df['year']==start_year[i]/10000].values[0]
        total_damages_norm[i] = total_damages[i]*baserate/yearrate
    except:
        pass

# For 2020 storms normalize by 2019, because PWT stops at 2019
for i in np.where(start_year/10000 == 2020):
    yearrate = pwt_df['rnna'][pwt_df['year']==2019].values[0]
    total_damages_norm[i] = total_damages[i]*baserate/yearrate


# # Plot comparing asset losses from TCs vs known damages

# In[14]:


# Load Philippines province masks

d_mask = xr.open_dataset(root_dir+'/REGION_MASKS/philippines_province_masks_newlitpopgrid.nc')
d_mask_wind = xr.open_dataset(root_dir+'/REGION_MASKS/philippines_province_masks_windfield.nc')

pickle_in = open(root_dir+"/REGION_MASKS/state_ids.p","rb")
state_ids = pickle.load(pickle_in)


# In[38]:


def TDR(mod,obs): # inputs are paired modeled and observed losses
    tdr = np.sum(mod)/np.sum(obs) # total damage ratio-- closest to 1 is optimal
    return tdr

def RMSF(mod,obs): # inputs are paired modeled and observed losses
    edr = mod/obs # event damage ratio
    edr_no0 = edr[np.where(edr>0)] # exclude 0 points because can't calculate log for those
    rmsf = np.exp((np.mean(np.log(edr_no0)**2))**0.5) # root-mean squared fraction-- lower = better fit
    return rmsf, edr

def corr_metrics(mod,obs):
    pr = pearsonr(mod, obs)[0]
    ktau = kendalltau(mod, obs)[0]
    sr = spearmanr(mod, obs)[0]
    tdr = TDR(mod, obs)
    rmsf, edr = RMSF(mod, obs)
    return pr, ktau, sr, tdr, rmsf, edr

def corr_metrics2(mod,obs):
    tdr = TDR(mod, obs)
    rmsf, edr = RMSF(mod, obs)
    return tdr, rmsf

# In[39]:


#%%time
# Plot comparing simulated damages to observed damages
Vthresh = np.arange(15,40,5)# Eberenz et al 2020, 25.7 is value used for all and 10 below and above that; m/s
Vhalf = np.arange(50,210,10)# Eberenz et al 2020, value for Philippines using default, RMSF calculation, and TDR calculation; m/s

obs_paired_losses = {}
mod_paired_losses = {}
mod_losses = {}
obs_paired_losses_reg = defaultdict(dict)
mod_paired_losses_reg = defaultdict(dict)
mod_losses_reg = defaultdict(dict)
year_reg = defaultdict(dict)
pr = defaultdict(dict)
ktau = defaultdict(dict)
sr = defaultdict(dict)
tdr = defaultdict(dict)
rmsf = defaultdict(dict)
edr = defaultdict(dict)

for vt in Vthresh:
    for vh in Vhalf:
        
        # Calculate modeled damages
        label = 'Vhalf-'+str(vh)+'_Vthresh-'+str(vt)
        file_losses = '/ASSET_LOSSES/IBTrACS/VCURVE_SENSITIVITY/'+label+'.nc'
        ds_losses = xr.open_dataset(direc + file_losses)
        mod_losses[label] = ds_losses.sum(dim=['lon','lat']).asset_losses.values

        # Pair observed and modeled damages
        obs_paired_losses[label] = total_damages_norm[matches[:,0]]
        mod_paired_losses[label] = mod_losses[label][matches[:,1]]
        
        # Calculate overall metrics of correlation
        tdr[label]['all'], rmsf[label]['all'] = corr_metrics2(mod_paired_losses[label], obs_paired_losses[label])

        # Select storms that have positive asset losses in region 
        # Note: could also just select ones that have positive windspeed, which is what Eberenz does
        est_cost_reg = {}
        for reg in state_ids.keys():
            print('Vthresh = '+str(vt)+', Vhalf='+str(vh)+', reg = '+reg)
            mask0 = d_mask.states.where(d_mask.states==state_ids[reg])
            mask0 = mask0/mask0
            mask = np.repeat(np.expand_dims(mask0,axis=0), np.shape(ds_losses.asset_losses)[0], axis=0)
            mod_losses_inreg = (ds_losses.asset_losses*mask).sum(dim=['lat','lon']).values
            # Indices When Province Affected
            affected_assets = np.where(mod_losses_inreg>0) #indices of positive losses for all data
            affected_assets_match = np.where(mod_losses_inreg[matches[:,1]]>0) # indices of positive losses for only matched data
            # Paired Indices When Province Affected
            mod_paired_losses_reg[label][reg] = mod_paired_losses[label][affected_assets_match]
            obs_paired_losses_reg[label][reg] = obs_paired_losses[label][affected_assets_match]
            mod_losses_reg[label][reg] = mod_losses[label][affected_assets]
            year_reg[label][reg] = year[affected_assets]
        
            # Calculate regional of correlation
            tdr[label][reg], rmsf[label][reg] = corr_metrics2(mod_paired_losses_reg[label][reg], obs_paired_losses_reg[label][reg])

pickle.dump( tdr, open( root_dir+"/ASSET_LOSSES/CORR_METRICS/tdr_2.p", "wb" ) )
pickle.dump( rmsf, open( root_dir+"/ASSET_LOSSES/CORR_METRICS/rmsf_2.p", "wb" ) )

# Report on timing        
print("My program took", time.time() - start_time, "to run.")

